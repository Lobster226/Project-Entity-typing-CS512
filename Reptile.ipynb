{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ybu-RqM8MhDa","executionInfo":{"status":"ok","timestamp":1670888359115,"user_tz":360,"elapsed":17800,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"2cfde364-986f-401d-dc62-7fc3d1d119ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd \"drive/MyDrive/src\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vtdZ8YfAmg0-","executionInfo":{"status":"ok","timestamp":1670888361364,"user_tz":360,"elapsed":163,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"2a277fb6-9613-4fde-a94c-36e4b2453d1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/19SQ4Q8TMprp6D7i0D29mHcTJOj0FYWuu/src\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rB15co1miWR","executionInfo":{"status":"ok","timestamp":1670888364110,"user_tz":360,"elapsed":436,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"999ad7dd-4257-4bac-be51-4ed08b6820b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["embedding_2.npy\t\t      reptile.py\n","embedding_label.npz\t      split_dataset.py\n","entity_embedding_mapping.txt  type_dict.txt\n","entity_type_embedding.txt     YAGO4-class.txt\n","meta.py\t\t\t      YAGO4ET20-test.txt\n","__pycache__\t\t      YAGO4ET20-train.txt\n","Reptile.ipynb\t\t      YAGO4-types-freq20-single-mapping.txt\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.insert(0,'/content/drive/My Drive/Courses/CS 543/Final Proj/')\n"],"metadata":{"id":"IRUe6VozmqG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@formatter:off\n","%load_ext autoreload\n","%autoreload 2\n","#@formatter:on"],"metadata":{"id":"4Kkf_RC_muhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZK1GlYIfmvn9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Read in dataset"],"metadata":{"id":"ezluR5rUmwvj"}},{"cell_type":"code","source":["import numpy as np\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn import Linear\n","from copy import deepcopy\n","\n","# X = np.load('entity_embedding_200.npy')\n","# y = np.load('type.npy')\n","\n","\n","# load type_dict.txt as two dictionary\n","## type_dict_idxtoname: idex (int) -> type name\n","## type_dict_nametoidx: type name -> index (int)\n","with open(\"type_dict.txt\", encoding=\"UTF-8\") as f:\n","    type_dict_idxtoname = {}\n","    type_dict_nametoidx = {}\n","    aa = f.readline().strip()\n","    while aa:\n","        type_idx, type_name = aa.split('\\t')\n","        type_dict_idxtoname[int(type_idx)] = type_name\n","        type_dict_nametoidx[type_name] = int(type_idx)\n","        aa = f.readline().strip()\n","        \n","\n","# entity name to index mapping\n","entity_to_index = {}\n","with open('entity_embedding_mapping.txt', 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        e, i = line.strip().split('\\t')\n","        i = int(i)\n","        entity_to_index[e] = i\n","\n","# types\n","types = set()\n","with open('YAGO4ET20-train.txt', 'r', encoding='utf-8') as f:\n","    lines = f.readlines()\n","    for line in lines:\n","        e, t = line.strip().split('\\t')\n","        types.add(t)\n","types = list(types)\n","\n","# load data        \n","data_npz = np.load('embedding_label.npz')\n","X = data_npz['embedding']\n","y = data_npz['label']\n","\n","# meta task generation\n","\n","import random \n","import meta\n","\n","def sample_meta_task(n_shots=5, type_name=None):\n","    if type_name is None:\n","        type_name = random.choice(types)\n","    pos, neg = meta.generate_meta_task(type_name)\n","    n_shots = min([n_shots, len(pos), len(neg)])\n","    pos = random.sample(pos, n_shots)\n","    neg = random.sample(neg, n_shots)\n","    pos = np.array([entity_to_index[e] for e in pos])\n","    neg = np.array([entity_to_index[e] for e in neg])\n","    features = np.vstack([X[pos], X[neg]])\n","    labels =  np.concatenate((\n","        np.ones(len(pos)), \n","        np.zeros(len(neg))))\n","    return features, labels\n","\n","def get_test_task(type_name, test_compatible_types=False):\n","    train_features, train_labels = sample_meta_task(type_name=type_name)\n","    pos = meta.get_test_positive_examples(\n","        type_name, \n","        test_compatible_types=test_compatible_types)\n","    pos = [entity_to_index[e] for e in pos]\n","    test_features = X[pos]\n","    return train_features, train_labels, test_features\n"],"metadata":{"id":"lkujZGWnm0PR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model \n","\n","\n","INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE = 400, 1024, 1\n","\n","class Model(nn.Module):\n","    def __init__(self, weights=None):\n","        super().__init__()\n","        self.fc1 = Linear(INPUT_SIZE, HIDDEN_SIZE)\n","        self.fc2 = Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n","        self.fc3 = Linear(HIDDEN_SIZE, HIDDEN_SIZE)\n","        self.out = Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n","\n","        # This has to be after the weight initializations or else we get a\n","        # KeyError.\n","        if weights is not None:\n","            self.load_state_dict(deepcopy(weights))\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        return F.sigmoid(self.out(x).view(-1))\n","\n","def inner_train_func(model, x, y, n_iter, log_period=1, **kwargs):\n","    for epoch in range(n_iter):  # loop over the dataset multiple times\n","        running_loss = 0.0\n","        \n","        inputs, labels = x, y\n","        #inputs, labels = data\n","        #criterion = nn.CrossEntropyLoss()\n","        criterion =  nn.BCELoss(reduction='mean')\n","        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if log_period is not None:\n","            if epoch % log_period == log_period-1:    # print every log_period mini-batches\n","                print(f'[{epoch + 1}, {epoch + 1:5d}] loss: {running_loss / log_period:.3f}')\n","                running_loss = 0.0\n","\n","def copy_model(original_model):\n","    import copy\n","    return copy.deepcopy(original_model)"],"metadata":{"id":"hfaPNJmSnBzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def parse_args(args=None):\n","    parser = argparse.ArgumentParser(\n","        description='Meta Learning Arguments',\n","        usage='train.py [<args>] [-h | --help]'\n","    )\n","\n","    parser.add_argument('--cuda', action='store_true', help='use GPU')\n","    parser.add_argument('--data_path', type=str, default=None)\n","    parser.add_argument('--model', default='Reptile', type=str)\n","    #parser.add_argument('-b', '--batch_size', default=1024, type=int)\n","    parser.add_argument('-lr', '--learning_rate', default=0.0001, type=float)\n","    parser.add_argument('--n_shot', default=5, type=int)\n","    parser.add_argument('-save', '--save_path', default=None, type=str)\n","    parser.add_argument('--outer_loops', default=10, type=int)\n","    parser.add_argument('--inner_loops', default=1000, type=int)\n","    parser.add_argument('--log_steps', default=100, type=int, help='train log every xx steps')\n","    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n","    \n","    return parser.parse_args(args)\n","def reptile_train(model, device, n_shot,\n","                  n_iter_meta, meta_step_size,\n","                  inner_train_func, n_iter_inner=1000,\n","                  log_period_meta=10, log_period_inner=1):\n","    # Reptile training loop\n","    for iteration in range(n_iter_meta):\n","        weights_before = deepcopy(model.state_dict())\n","        # Generate task \n","        x, y = sample_meta_task(n_shot)\n","        x = torch.from_numpy(x.astype(np.float32)).to(device)\n","        y = torch.from_numpy(y.astype(np.float32)).to(device)\n","        # Do optimization on this task\n","        if iteration % log_period_meta == 0:\n","            print('Meta iter', iteration, ': ')\n","        inner_train_func(model, x, y, n_iter=n_iter_inner,\n","                         log_period=log_period_inner \n","                            if iteration % log_period_meta == 0 else None)\n","        # Interpolate between current weights and trained weights from this task\n","        # I.e. (weights_before - weights_after) is the meta-gradient\n","        weights_after = model.state_dict()\n","        step_size = meta_step_size * (\n","                1 - iteration / n_iter_meta)  # linear schedule\n","        model.load_state_dict(\n","            {name: weights_before[name] + (weights_after[name] -\n","                                           weights_before[name]) * step_size\n","             for name in weights_before})\n"],"metadata":{"id":"ppvlHzjQnGsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = Model().to(device)\n","#meta_weights = model.state_dict()\n","reptile_train(\n","    model, device, 100, 10000, meta_step_size=0.5, \n","    inner_train_func=inner_train_func, \n","    n_iter_inner=20,\n","    log_period_meta=500, log_period_inner=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vs1iMAVzxjaF","executionInfo":{"status":"ok","timestamp":1670828400527,"user_tz":360,"elapsed":669345,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"6390d473-7ba5-4b16-c281-b694f09fe470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Meta iter 0 : \n","[5,     5] loss: 0.139\n","[10,    10] loss: 0.138\n","[15,    15] loss: 0.138\n","[20,    20] loss: 0.138\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["Meta iter 500 : \n","[5,     5] loss: 0.175\n","[10,    10] loss: 0.058\n","[15,    15] loss: 0.031\n","[20,    20] loss: 0.022\n","Meta iter 1000 : \n","[5,     5] loss: 0.001\n","[10,    10] loss: 0.001\n","[15,    15] loss: 0.001\n","[20,    20] loss: 0.001\n","Meta iter 1500 : \n","[5,     5] loss: 0.005\n","[10,    10] loss: 0.003\n","[15,    15] loss: 0.002\n","[20,    20] loss: 0.002\n","Meta iter 2000 : \n","[5,     5] loss: 0.182\n","[10,    10] loss: 0.018\n","[15,    15] loss: 0.011\n","[20,    20] loss: 0.008\n","Meta iter 2500 : \n","[5,     5] loss: 0.005\n","[10,    10] loss: 0.003\n","[15,    15] loss: 0.002\n","[20,    20] loss: 0.002\n","Meta iter 3000 : \n","[5,     5] loss: 0.004\n","[10,    10] loss: 0.003\n","[15,    15] loss: 0.002\n","[20,    20] loss: 0.002\n","Meta iter 3500 : \n","[5,     5] loss: 0.000\n","[10,    10] loss: 0.000\n","[15,    15] loss: 0.000\n","[20,    20] loss: 0.000\n","Meta iter 4000 : \n","[5,     5] loss: 0.001\n","[10,    10] loss: 0.001\n","[15,    15] loss: 0.001\n","[20,    20] loss: 0.001\n","Meta iter 4500 : \n","[5,     5] loss: 0.156\n","[10,    10] loss: 0.008\n","[15,    15] loss: 0.005\n","[20,    20] loss: 0.004\n","Meta iter 5000 : \n","[5,     5] loss: 0.033\n","[10,    10] loss: 0.003\n","[15,    15] loss: 0.003\n","[20,    20] loss: 0.002\n","Meta iter 5500 : \n","[5,     5] loss: 0.027\n","[10,    10] loss: 0.004\n","[15,    15] loss: 0.003\n","[20,    20] loss: 0.002\n","Meta iter 6000 : \n","[5,     5] loss: 0.002\n","[10,    10] loss: 0.001\n","[15,    15] loss: 0.001\n","[20,    20] loss: 0.001\n","Meta iter 6500 : \n","[5,     5] loss: 0.003\n","[10,    10] loss: 0.002\n","[15,    15] loss: 0.002\n","[20,    20] loss: 0.002\n","Meta iter 7000 : \n","[5,     5] loss: 0.002\n","[10,    10] loss: 0.001\n","[15,    15] loss: 0.001\n","[20,    20] loss: 0.001\n","Meta iter 7500 : \n","[5,     5] loss: 0.113\n","[10,    10] loss: 0.010\n","[15,    15] loss: 0.007\n","[20,    20] loss: 0.006\n","Meta iter 8000 : \n","[5,     5] loss: 0.001\n","[10,    10] loss: 0.001\n","[15,    15] loss: 0.001\n","[20,    20] loss: 0.001\n","Meta iter 8500 : \n","[5,     5] loss: 0.002\n","[10,    10] loss: 0.002\n","[15,    15] loss: 0.002\n","[20,    20] loss: 0.001\n","Meta iter 9000 : \n","[5,     5] loss: 0.082\n","[10,    10] loss: 0.007\n","[15,    15] loss: 0.005\n","[20,    20] loss: 0.004\n","Meta iter 9500 : \n","[5,     5] loss: 0.070\n","[10,    10] loss: 0.023\n","[15,    15] loss: 0.009\n","[20,    20] loss: 0.007\n"]}]},{"cell_type":"code","source":["correct = 0\n","total = 0\n","acc = 0\n","# again no gradients needed\n","\n","for t in types:\n","    train_features, train_labels, test_features = get_test_task(t, False)\n","    #images, labels = data\n","    #inputs, labels = data[0].to(device), data[1].to(device)\n","    x = torch.from_numpy(train_features.astype(np.float32)).to(device)\n","    y = torch.from_numpy(train_labels.astype(np.float32)).to(device)\n","    # Do optimization on this task\n","    tmp_model = copy_model(model).to(device)\n","    inner_train_func(tmp_model, x, y, n_iter=20,\n","                    log_period=None)\n","    with torch.no_grad():\n","      outputs = tmp_model(torch.from_numpy(test_features).to(device)).detach()\n","      predictions = outputs > 0.5\n","      n_examples = test_features.shape[0]\n","      n_correct = torch.sum(predictions.double()).item()\n","      correct += n_correct\n","      total += n_examples\n","      predictions.size\n","      acc += torch.mean(predictions.double()).item() # ground truth are all positive\n","\n","    # _, predictions = torch.max(outputs, 1)\n","    # # collect the correct predictions for each class\n","    # for label, prediction in zip(labels, predictions):\n","    #     total += 1\n","    #     if label == prediction:\n","    #         correct += 1\n","print(f'Accuracy of the network: {100 * acc / len(types)} %')\n","print(f'Accuracy of the network (by data count): {100 * correct / total} %')\n","print(len(types), total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDahPDDG1YGF","executionInfo":{"status":"ok","timestamp":1670828702648,"user_tz":360,"elapsed":23815,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"8d86d23a-f9f9-405d-c0a0-bc0dad0cf4ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network: 15.24202393716967 %\n","Accuracy of the network (by data count): 64.9197230814553 %\n","311 33945\n"]}]},{"cell_type":"code","source":["correct = 0\n","total = 0\n","acc = 0\n","# again no gradients needed\n","\n","for t in types:\n","    train_features, train_labels, test_features = get_test_task(\n","        t, test_compatible_types=True)\n","    #images, labels = data\n","    #inputs, labels = data[0].to(device), data[1].to(device)\n","    x = torch.from_numpy(train_features.astype(np.float32)).to(device)\n","    y = torch.from_numpy(train_labels.astype(np.float32)).to(device)\n","    # Do optimization on this task\n","    tmp_model = copy_model(model).to(device)\n","    inner_train_func(tmp_model, x, y, n_iter=20,\n","                    log_period=None)\n","    with torch.no_grad():\n","      outputs = tmp_model(torch.from_numpy(test_features).to(device)).detach()\n","      predictions = outputs > 0.5\n","      n_examples = test_features.shape[0]\n","      n_correct = torch.sum(predictions.double()).item()\n","      correct += n_correct\n","      total += n_examples\n","      predictions.size\n","      acc += torch.mean(predictions.double()).item() # ground truth are all positive\n","\n","    # _, predictions = torch.max(outputs, 1)\n","    # # collect the correct predictions for each class\n","    # for label, prediction in zip(labels, predictions):\n","    #     total += 1\n","    #     if label == prediction:\n","    #         correct += 1\n","print(f'Accuracy of the network: {100 * acc / len(types)} %')\n","print(f'Accuracy of the network (by data count): {100 * correct / total} %')\n","print(len(types), total)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NTkgQnexE4yI","executionInfo":{"status":"ok","timestamp":1670828739614,"user_tz":360,"elapsed":22847,"user":{"displayName":"YQ Bao","userId":"13767238705368968505"}},"outputId":"5e1ba0c2-4a62-4af0-b2e0-6b2989da5003"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network: 15.614925726322706 %\n","Accuracy of the network (by data count): 38.11165120047135 %\n","311 33945\n"]}]},{"cell_type":"code","source":["print(X.shape, max(entity_to_index.values()), len(entity_to_index))"],"metadata":{"id":"HuUgOeqLBlt5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"sLtBucyvnWEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a, b = meta.generate_meta_task('http://yago-knowledge.org/resource/Human')\n","for t in a:\n","  assert t in entity_to_index\n","for t in b:\n","  assert t in entity_to_index"],"metadata":{"id":"WRxSnMmnydZO"},"execution_count":null,"outputs":[]}]}